% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/apg.R
\name{apg}
\alias{apg}
\title{Accelerated proximal gradient optimization}
\usage{
apg(grad_f, prox_h, dim_x, opts)
}
\arguments{
\item{grad_f}{A function that computes the gradient of f :
\eqn{grad_f(v,opts) = df(v)/dv}}

\item{prox_h}{A function that computes the proximal operator of h :
\eqn{prox_h(v,t,opts) = argmin_x (t*h(x) + 1/2 * norm(x-v)^2)}}

\item{dim_x}{The dimension of the unknown \eqn{x}}

\item{opts}{List of parameters, both for the \code{apg} function and for the
  \code{grad_f} and \code{prox_h} functions. For \code{apg}, the list can
  contain the following fields: \itemize{ \item \code{X_INIT}: initial
  starting point (default \code{0}) \item \code{USE_RESTART} : use adaptive
  restart scheme (default \code{TRUE}) \item \code{MAX_ITERS} : maximum
  iterations before termination (default \code{2000}) \item \code{EPS} :
  tolerance for termination (default \code{1e-6}) \item \code{ALPHA} :
  step-size growth factor (default \code{1.01}) \item \code{BETA} : step-size
  shrinkage factor (default \code{0.5}) \item \code{QUIET} : if false writes
  out information every 100 iters (default \code{FALSE}) \item
  \code{GEN_PLOTS} : if true generates plots of norm of proximal gradient
  (default \code{TRUE}) \item \code{USE_GRA} : if true uses UN-accelerated
  proximal gradient descent (typically slower) (default \code{FALSE}) \item
  \code{STEP_SIZE} : starting step-size estimate, if not set then apg makes
  initial guess (default \code{NULL}) \item \code{FIXED_STEP_SIZE} : don't
  change step-size (forward or back tracking), uses initial step-size
  throughout, only useful if good STEP_SIZE set (default \code{FALSE}) } In
  addition, \code{opts} can contain other fields that will be passed to the
  \code{grad_f} and \code{prox_h} functions. This code was borrowed and
  adapted from the MATLAB version of Brendan O'Donoghue available at
  \code{https://github.com/bodono/apg}}
}
\value{
A list with \code{x}, the solution of the problem: \deqn{min_x (f(x)
  + h(x)), x \in R^dim_x ,} and \code{t}, the last step size.
}
\description{
This function implements an accelerated proximal gradient method (Nesterov
2007, Beck and Teboulle 2009). It solves: \deqn{min_x (f(x) + h(x)), x \in
R^dim_x} where \eqn{f} is smooth, convex and \eqn{h} is non-smooth, convex
but simple in that we can easily evaluate the proximal operator of h.
}
\examples{
# Solve a Lasso problem:
# min_x 1/2 norm( A\%*\%x - b )^2 + lambda ||x||_1
n <- 50
m <- 20
lambda <- 1
A <- matrix(rnorm(m*n), nrow=n)
b <- rnorm(n)
r <- apg(grad.quad, prox.l1, m, list(A=A, b=b, lambda=lambda) )
# This gives the same result as:
# m <- glmnet(A,b,alpha=1, standardize=FALSE,lambda=1/50,intercept=FALSE)
}

